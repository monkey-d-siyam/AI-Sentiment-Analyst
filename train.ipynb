{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Efficient Multi-class Text Classification Project\n",
    "\n",
    "This notebook implements a complete pipeline for multi-class text classification including:\n",
    "1. **Data Handling**: Merging 5 training files, cleaning duplicates/missing values.\n",
    "2. **Preprocessing**: Modular and vectorized text cleaning.\n",
    "3. **Representations**: BoW, TF-IDF, GloVe, and Word2Vec Skip-gram.\n",
    "4. **ML Models**: Logistic Regression, Naive Bayes, Random Forest with Hyperparameter Tuning.\n",
    "5. **NN Models**: DNN, RNN, GRU, LSTM, Bidirectional architectures.\n",
    "6. **Evaluation**: Metrics and Visualization.\n",
    "7. **Deployment Readiness**: Saving all artifacts for the Flask app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, SimpleRNN, Dense, Dropout, Bidirectional, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from text_pipeline import (\n",
    "    load_and_merge_data, preprocess_dataframe, build_tokenizer, \n",
    "    get_glove_embeddings, train_word2vec_skipgram, build_w2v_matrix, \n",
    "    save_artifact, load_artifact\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Ensure models directory exists\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## 1. Data Handling & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glob = 'Datasets/*[Training]*.csv'\n",
    "test_path = 'Datasets/[Updated] Question Answer Classification Dataset[Test] (1).csv'\n",
    "\n",
    "train_df, test_df = load_and_merge_data(train_glob, test_path)\n",
    "print(f\"Total Training Samples: {len(train_df)}\")\n",
    "print(f\"Total Test Samples: {len(test_df)}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing Training Data...\")\n",
    "train_df = preprocess_dataframe(train_df)\n",
    "print(\"Preprocessing Test Data...\")\n",
    "test_df = preprocess_dataframe(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "### ML Features: BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "count_vect = CountVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "tfidf_vect = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "\n",
    "X_train_bow = count_vect.fit_transform(train_df['clean_text'])\n",
    "X_test_bow = count_vect.transform(test_df['clean_text'])\n",
    "save_artifact(count_vect, 'models/count_vect.pkl')\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(train_df['clean_text'])\n",
    "X_test_tfidf = tfidf_vect.transform(test_df['clean_text'])\n",
    "save_artifact(tfidf_vect, 'models/tfidf_vect.pkl')\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "save_artifact(le, 'models/label_encoder.pkl')\n",
    "\n",
    "print(\"BoW Shape:\", X_train_bow.shape)\n",
    "print(\"TF-IDF Shape:\", X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word_reps",
   "metadata": {},
   "source": [
    "### Neural Network Word Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nn_reps",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "maxlen = 100\n",
    "tokenizer, X_train_seq = build_tokenizer(train_df['clean_text'], num_words=max_words, maxlen=maxlen)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['clean_text']), maxlen=maxlen, padding='post', truncating='post')\n",
    "save_artifact(tokenizer, 'models/tokenizer.pkl')\n",
    "\n",
    "# 1. Word2Vec Skip-gram\n",
    "w2v_model = train_word2vec_skipgram(train_df['clean_text'])\n",
    "embedding_matrix_w2v = build_w2v_matrix(tokenizer.word_index, w2v_model)\n",
    "np.save('models/embedding_matrix_w2v.npy', embedding_matrix_w2v)\n",
    "\n",
    "# 2. GloVe (Optional - set glove_path if available)\n",
    "# glove_path = 'glove.6B.100d.txt'\n",
    "# embedding_matrix_glove = get_glove_embeddings(tokenizer.word_index, glove_path=glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml_models",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Models\n",
    "Testing Logistic Regression, Naive Bayes, and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_ml",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_results = []\n",
    "\n",
    "def evaluate_ml(model, X_test, y_test, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")\n",
    "    return {'name': name, 'accuracy': acc, 'f1': f1, 'model': model}\n",
    "\n",
    "# LogReg Tuning\n",
    "print(\"Tuning Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "param_grid_lr = {'C': [0.1, 1, 10]}\n",
    "grid_lr = GridSearchCV(lr, param_grid_lr, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "grid_lr.fit(X_train_tfidf, y_train)\n",
    "ml_results.append(evaluate_ml(grid_lr.best_estimator_, X_test_tfidf, y_test, \"LogReg-TFIDF\"))\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"Training Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train)\n",
    "ml_results.append(evaluate_ml(nb, X_test_bow, y_test, \"NaiveBayes-BoW\"))\n",
    "\n",
    "# Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, n_jobs=-1)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "ml_results.append(evaluate_ml(rf, X_test_tfidf, y_test, \"RandomForest-TFIDF\"))\n",
    "\n",
    "# Save best ML model\n",
    "best_ml = max(ml_results, key=lambda x: x['f1'])\n",
    "save_artifact(best_ml['model'], 'models/best_ml_model.pkl')\n",
    "print(f\"Best ML Model: {best_ml['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn_models",
   "metadata": {},
   "source": [
    "## 4. Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_nn",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(le.classes_)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "\n",
    "def get_model(arch='lstm', use_embeddings=True):\n",
    "    model = Sequential()\n",
    "    if use_embeddings and embedding_matrix_w2v is not None:\n",
    "        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix_w2v], input_length=maxlen, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "        \n",
    "    if arch == 'dnn':\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "    elif arch == 'rnn':\n",
    "        model.add(SimpleRNN(64))\n",
    "    elif arch == 'lstm':\n",
    "        model.add(LSTM(64))\n",
    "    elif arch == 'gru':\n",
    "        model.add(GRU(64))\n",
    "    elif arch == 'bilstm':\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training a sample LSTM (Highly efficient)\n",
    "print(\"Training LSTM Model...\")\n",
    "nn_model = get_model('lstm')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = nn_model.fit(X_train_seq, y_train_enc, \n",
    "                       validation_data=(X_test_seq, y_test_enc), \n",
    "                       epochs=10, batch_size=128, callbacks=[early_stop])\n",
    "\n",
    "nn_model.save('models/best_nn_model.h5')\n",
    "print(\"NN Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## 5. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_nn = np.argmax(nn_model.predict(X_test_seq), axis=1)\n",
    "y_pred_labels = le.inverse_transform(y_pred_nn)\n",
    "\n",
    "print(\"Classification Report (Best NN):\")\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
